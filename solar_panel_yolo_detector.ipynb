{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solar Panel Fault Detection using YOLO\n",
    "\n",
    "This notebook implements a YOLO-based detector for identifying faults in solar panels using thermal imaging.\n",
    "\n",
    "Key fault types detected:\n",
    "- **Hotspots**: Single heated cells indicating issues like shading or soiling\n",
    "- **Cracks**: Microcracks in solar cells caused by manufacturing stress or environmental factors\n",
    "- **Shadings**: Cold spots caused by external objects blocking sunlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import glob\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, LeakyReLU, MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Reshape, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "\n",
    "This notebook is designed to work with the Photovoltaic Module Dataset (PVMD), which contains:\n",
    "- 1,000 thermal images of solar panels\n",
    "- Images collected using DJI Mavic 3 Thermal drone\n",
    "- Standardized to 512\u00d7512\u00d73 dimensions\n",
    "- Collected on September 5, 2024, at Tshwane University of Technology, South Africa\n",
    "\n",
    "The dataset is publicly available in the Mendeley data repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Detector Class Implementation\n",
    "\n",
    "Below is the implementation of our Solar Panel YOLO Detector class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SolarPanelYOLODetector:\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"\n",
    "        Initialize the Solar Panel YOLO Detector\n",
    "        \n",
    "        Parameters:\n",
    "        data_dir (str): Directory containing the YOLO dataset (images and labels)\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.img_size = (416, 416)  # Standard YOLO input size\n",
    "        self.classes = ['Hotspots', 'Cracks', 'Shadings']\n",
    "        self.num_classes = len(self.classes)\n",
    "        self.model = None\n",
    "        \n",
    "        # YOLO-specific parameters\n",
    "        self.anchors = np.array([\n",
    "            [10, 13], [16, 30], [33, 23],  # Small objects\n",
    "            [30, 61], [62, 45], [59, 119],  # Medium objects\n",
    "            [116, 90], [156, 198], [373, 326]  # Large objects\n",
    "        ])\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.yolo_outputs = 3  # Number of YOLO output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data Loading Methods\n",
    "def parse_yolo_annotation(self, annotation_path):\n",
    "    \"\"\"\n",
    "    Parse YOLO format annotation file\n",
    "    \n",
    "    Parameters:\n",
    "    annotation_path (str): Path to the annotation file\n",
    "    \n",
    "    Returns:\n",
    "    list: List of bounding boxes in the format [class_id, x, y, w, h]\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data = line.strip().split(' ')\n",
    "            if len(data) == 5:  # class_id, x_center, y_center, width, height\n",
    "                class_id = int(data[0])\n",
    "                x_center = float(data[1])\n",
    "                y_center = float(data[2])\n",
    "                width = float(data[3])\n",
    "                height = float(data[4])\n",
    "                boxes.append([class_id, x_center, y_center, width, height])\n",
    "    return boxes\n",
    "\n",
    "def load_yolo_dataset(self, split_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Load YOLO dataset from the given directory\n",
    "    \n",
    "    Parameters:\n",
    "    split_ratio (float): Train/test split ratio\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (train_images, train_annotations, test_images, test_annotations)\n",
    "    \"\"\"\n",
    "    print(\"Loading YOLO dataset...\")\n",
    "    \n",
    "    # Find all image files\n",
    "    image_paths = glob.glob(os.path.join(self.data_dir, \"images\", \"*.jpg\"))\n",
    "    image_paths.extend(glob.glob(os.path.join(self.data_dir, \"images\", \"*.png\")))\n",
    "    \n",
    "    # Shuffle the image paths\n",
    "    random.shuffle(image_paths)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    split_index = int(len(image_paths) * split_ratio)\n",
    "    train_images = image_paths[:split_index]\n",
    "    test_images = image_paths[split_index:]\n",
    "    \n",
    "    # Get corresponding annotation files\n",
    "    train_annotations = []\n",
    "    for img_path in train_images:\n",
    "        base_name = os.path.basename(img_path)\n",
    "        name_without_ext = os.path.splitext(base_name)[0]\n",
    "        annotation_path = os.path.join(self.data_dir, \"labels\", f\"{name_without_ext}.txt\")\n",
    "        if os.path.exists(annotation_path):\n",
    "            train_annotations.append(annotation_path)\n",
    "        else:\n",
    "            print(f\"Warning: No annotation file for {img_path}\")\n",
    "    \n",
    "    test_annotations = []\n",
    "    for img_path in test_images:\n",
    "        base_name = os.path.basename(img_path)\n",
    "        name_without_ext = os.path.splitext(base_name)[0]\n",
    "        annotation_path = os.path.join(self.data_dir, \"labels\", f\"{name_without_ext}.txt\")\n",
    "        if os.path.exists(annotation_path):\n",
    "            test_annotations.append(annotation_path)\n",
    "        else:\n",
    "            print(f\"Warning: No annotation file for {img_path}\")\n",
    "    \n",
    "    print(f\"Dataset loaded: {len(train_images)} training images, {len(test_images)} testing images\")\n",
    "    return train_images, train_annotations, test_images, test_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocessing Methods\n",
    "def preprocess_image(self, img_path):\n",
    "    \"\"\"\n",
    "    Preprocess a single image for YOLO input\n",
    "    \n",
    "    Parameters:\n",
    "    img_path (str): Path to the image file\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Preprocessed image\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "    \n",
    "    # Resize to YOLO input size\n",
    "    img = cv2.resize(img, self.img_size)\n",
    "    \n",
    "    # Convert to RGB (OpenCV uses BGR by default)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "    return img\n",
    "\n",
    "def preprocess_batch(self, image_paths, annotation_paths):\n",
    "    \"\"\"\n",
    "    Preprocess a batch of images and annotations for YOLO training\n",
    "    \n",
    "    Parameters:\n",
    "    image_paths (list): List of image file paths\n",
    "    annotation_paths (list): List of annotation file paths\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (images, yolo_outputs)\n",
    "    \"\"\"\n",
    "    batch_size = len(image_paths)\n",
    "    images = np.zeros((batch_size, self.img_size[0], self.img_size[1], 3), dtype=np.float32)\n",
    "    \n",
    "    # Output grid sizes for the 3 YOLO output layers\n",
    "    grid_sizes = [self.img_size[0] // s for s in [32, 16, 8]]  # 13, 26, 52 for 416x416 input\n",
    "    \n",
    "    # Initialize YOLO outputs (one for each scale)\n",
    "    y_true = [\n",
    "        np.zeros((batch_size, grid_sizes[i], grid_sizes[i], \n",
    "                  self.num_anchors // 3, 5 + self.num_classes), dtype=np.float32)\n",
    "        for i in range(self.yolo_outputs)\n",
    "    ]\n",
    "    \n",
    "    for i, (img_path, anno_path) in enumerate(zip(image_paths, annotation_paths)):\n",
    "        # Preprocess image\n",
    "        images[i] = self.preprocess_image(img_path)\n",
    "        \n",
    "        # Parse annotation\n",
    "        boxes = self.parse_yolo_annotation(anno_path)\n",
    "        \n",
    "        # Assign boxes to the appropriate output grid and anchor\n",
    "        for box in boxes:\n",
    "            class_id, x_center, y_center, width, height = box\n",
    "            \n",
    "            # Determine which anchor box and output grid this box belongs to\n",
    "            # (This is a simplified version - actual YOLO would calculate IoU with anchors)\n",
    "            box_area = width * height\n",
    "            anchor_areas = self.anchors[:, 0] * self.anchors[:, 1] / (self.img_size[0] * self.img_size[1])\n",
    "            anchor_index = np.argmin(np.abs(anchor_areas - box_area))\n",
    "            \n",
    "            # Determine which output grid\n",
    "            output_index = anchor_index // 3\n",
    "            anchor_in_layer = anchor_index % 3\n",
    "            \n",
    "            # Calculate grid cell coordinates\n",
    "            grid_size = grid_sizes[output_index]\n",
    "            grid_x = int(x_center * grid_size)\n",
    "            grid_y = int(y_center * grid_size)\n",
    "            \n",
    "            # Ensure within bounds\n",
    "            grid_x = max(0, min(grid_x, grid_size - 1))\n",
    "            grid_y = max(0, min(grid_y, grid_size - 1))\n",
    "            \n",
    "            # Calculate target values\n",
    "            tx = x_center * grid_size - grid_x  # Offset within the grid cell\n",
    "            ty = y_center * grid_size - grid_y\n",
    "            tw = np.log(width * self.img_size[0] / self.anchors[anchor_index, 0])\n",
    "            th = np.log(height * self.img_size[1] / self.anchors[anchor_index, 1])\n",
    "            \n",
    "            # Set target values in the output grid\n",
    "            y_true[output_index][i, grid_y, grid_x, anchor_in_layer, 0:4] = [tx, ty, tw, th]\n",
    "            y_true[output_index][i, grid_y, grid_x, anchor_in_layer, 4] = 1  # Object confidence\n",
    "            y_true[output_index][i, grid_y, grid_x, anchor_in_layer, 5 + int(class_id)] = 1  # Class one-hot\n",
    "    \n",
    "    return images, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model Building Methods\n",
    "def build_yolo_model(self):\n",
    "    \"\"\"\n",
    "    Build a simplified YOLO model for solar panel fault detection\n",
    "    \n",
    "    Returns:\n",
    "    tensorflow.keras.models.Model: YOLO model\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_tensor = Input(shape=(self.img_size[0], self.img_size[1], 3))\n",
    "    \n",
    "    # Backbone\n",
    "    x = self._conv_block(input_tensor, 32, 3)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    x = self._conv_block(x, 64, 3)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    x = self._conv_block(x, 128, 3)\n",
    "    x = self._conv_block(x, 64, 1)\n",
    "    x = self._conv_block(x, 128, 3)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    x = self._conv_block(x, 256, 3)\n",
    "    x = self._conv_block(x, 128, 1)\n",
    "    x = self._conv_block(x, 256, 3)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    x = self._conv_block(x, 512, 3)\n",
    "    x = self._conv_block(x, 256, 1)\n",
    "    x = self._conv_block(x, 512, 3)\n",
    "    x = self._conv_block(x, 256, 1)\n",
    "    x = self._conv_block(x, 512, 3)\n",
    "    route_1 = x  # Route for feature map 1\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n",
    "    \n",
    "    x = self._conv_block(x, 1024, 3)\n",
    "    x = self._conv_block(x, 512, 1)\n",
    "    x = self._conv_block(x, 1024, 3)\n",
    "    x = self._conv_block(x, 512, 1)\n",
    "    x = self._conv_block(x, 1024, 3)\n",
    "    route_2 = x  # Route for feature map 2\n",
    "    \n",
    "    # YOLO head for large objects\n",
    "    large_objects = self._yolo_head(route_2, 512, self.num_anchors // 3 * (5 + self.num_classes))\n",
    "    \n",
    "    # Upsample and concatenate for medium objects\n",
    "    x = self._conv_block(route_2, 256, 1)\n",
    "    x = tf.keras.layers.UpSampling2D(2)(x)\n",
    "    x = Concatenate()([x, route_1])\n",
    "    x = self._conv_block(x, 256, 1)\n",
    "    x = self._conv_block(x, 512, 3)\n",
    "    medium_objects = self._yolo_head(x, 256, self.num_anchors // 3 * (5 + self.num_classes))\n",
    "    \n",
    "    # Reshape outputs to match YOLO format\n",
    "    large_grid_size = self.img_size[0] // 32\n",
    "    medium_grid_size = self.img_size[0] // 16\n",
    "    \n",
    "    large_output = Reshape((large_grid_size, large_grid_size, \n",
    "                           self.num_anchors // 3, 5 + self.num_classes))(large_objects)\n",
    "    medium_output = Reshape((medium_grid_size, medium_grid_size, \n",
    "                            self.num_anchors // 3, 5 + self.num_classes))(medium_objects)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=input_tensor, outputs=[large_output, medium_output])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=[self._yolo_loss, self._yolo_loss]\n",
    "    )\n",
    "    \n",
    "    self.model = model\n",
    "    return model\n",
    "\n",
    "def _conv_block(self, x, filters, kernel_size, strides=1):\n",
    "    \"\"\"\n",
    "    Create a convolution block with batch normalization and leaky ReLU\n",
    "    \"\"\"\n",
    "    x = Conv2D(filters, kernel_size, strides=strides, padding='same', \n",
    "               use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    return x\n",
    "\n",
    "def _yolo_head(self, x, filters, output_filters):\n",
    "    \"\"\"\n",
    "    Create a YOLO head for predictions\n",
    "    \"\"\"\n",
    "    x = self._conv_block(x, filters, 3)\n",
    "    x = Conv2D(output_filters, 1, padding='same')(x)\n",
    "    return x\n",
    "\n",
    "def _yolo_loss(self, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom YOLO loss function\n",
    "    \n",
    "    This is a simplified version of the YOLO loss function.\n",
    "    In a real implementation, you would calculate the full YOLO loss with:\n",
    "    - Object confidence loss\n",
    "    - No object confidence loss\n",
    "    - Box coordinate loss\n",
    "    - Class prediction loss\n",
    "    \"\"\"\n",
    "    # Mask for cells that contain objects\n",
    "    object_mask = y_true[..., 4:5]\n",
    "    \n",
    "    # Box coordinate loss (only for cells with objects)\n",
    "    box_loss = tf.reduce_sum(tf.square(y_true[..., 0:4] - y_pred[..., 0:4]), axis=-1, keepdims=True)\n",
    "    box_loss = object_mask * box_loss\n",
    "    \n",
    "    # Object confidence loss\n",
    "    confidence_loss = tf.square(y_true[..., 4:5] - y_pred[..., 4:5])\n",
    "    \n",
    "    # Class prediction loss\n",
    "    class_loss = object_mask * tf.reduce_sum(\n",
    "        tf.square(y_true[..., 5:] - y_pred[..., 5:]), axis=-1, keepdims=True\n",
    "    )\n",
    "    \n",
    "    # Combine losses\n",
    "    total_loss = box_loss + confidence_loss + class_loss\n",
    "    \n",
    "    return tf.reduce_mean(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training Methods\n",
    "def train_model(self, train_images, train_annotations, batch_size=8, epochs=50, validation_split=0.1):\n",
    "    \"\"\"\n",
    "    Train the YOLO model\n",
    "    \n",
    "    Parameters:\n",
    "    train_images (list): List of training image paths\n",
    "    train_annotations (list): List of training annotation paths\n",
    "    batch_size (int): Batch size for training\n",
    "    epochs (int): Number of epochs to train\n",
    "    validation_split (float): Fraction of data to use for validation\n",
    "    \n",
    "    Returns:\n",
    "    tensorflow.keras.callbacks.History: Training history\n",
    "    \"\"\"\n",
    "    if self.model is None:\n",
    "        self.build_yolo_model()\n",
    "    \n",
    "    # Split for validation\n",
    "    val_size = int(len(train_images) * validation_split)\n",
    "    val_images = train_images[-val_size:]\n",
    "    val_annotations = train_annotations[-val_size:]\n",
    "    train_images = train_images[:-val_size]\n",
    "    train_annotations = train_annotations[:-val_size]\n",
    "    \n",
    "    # Create data generators\n",
    "    train_generator = self._batch_generator(train_images, train_annotations, batch_size)\n",
    "    val_generator = self._batch_generator(val_images, val_annotations, batch_size)\n",
    "    \n",
    "    # Calculate steps per epoch\n",
    "    steps_per_epoch = len(train_images) // batch_size\n",
    "    validation_steps = len(val_images) // batch_size\n",
    "    \n",
    "    # Train model\n",
    "    history = self.model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=validation_steps\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def _batch_generator(self, image_paths, annotation_paths, batch_size):\n",
    "    \"\"\"\n",
    "    Generate batches of data for training\n",
    "    \"\"\"\n",
    "    n = len(image_paths)\n",
    "    while True:\n",
    "        for i in range(0, n, batch_size):\n",
    "            batch_images = image_paths[i:i+batch_size]\n",
    "            batch_annotations = annotation_paths[i:i+batch_size]\n",
    "            \n",
    "            # Pad the last batch if necessary\n",
    "            if len(batch_images) < batch_size:\n",
    "                batch_images = batch_images + [batch_images[0]] * (batch_size - len(batch_images))\n",
    "                batch_annotations = batch_annotations + [batch_annotations[0]] * (batch_size - len(batch_annotations))\n",
    "            \n",
    "            X, y = self.preprocess_batch(batch_images, batch_annotations)\n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Detection Methods\n",
    "def detect(self, img_path, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detect solar panel faults in an image\n",
    "    \n",
    "    Parameters:\n",
    "    img_path (str): Path to the image file\n",
    "    confidence_threshold (float): Threshold for object detection confidence\n",
    "    \n",
    "    Returns:\n",
    "    list: List of detected objects with class, confidence, and bounding box\n",
    "    \"\"\"\n",
    "    if self.model is None:\n",
    "        raise ValueError(\"Model is not trained. Call build_yolo_model() first.\")\n",
    "    \n",
    "    # Preprocess image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "    \n",
    "    orig_height, orig_width = img.shape[:2]\n",
    "    \n",
    "    # Preprocess for input\n",
    "    input_img = cv2.resize(img, self.img_size)\n",
    "    input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "    input_img = input_img.astype(np.float32) / 255.0\n",
    "    input_img = np.expand_dims(input_img, axis=0)\n",
    "    \n",
    "    # Predict\n",
    "    outputs = self.model.predict(input_img)\n",
    "    \n",
    "    # Process outputs\n",
    "    detections = []\n",
    "    \n",
    "    grid_sizes = [self.img_size[0] // s for s in [32, 16]]  # 13, 26 for 416x416 input\n",
    "    \n",
    "    for output_idx, output in enumerate(outputs):\n",
    "        output = output[0]  # Remove batch dimension\n",
    "        grid_size = grid_sizes[output_idx]\n",
    "        \n",
    "        for row in range(grid_size):\n",
    "            for col in range(grid_size):\n",
    "                for anchor_idx in range(self.num_anchors // 3):\n",
    "                    # Extract data\n",
    "                    tx, ty, tw, th = output[row, col, anchor_idx, 0:4]\n",
    "                    confidence = output[row, col, anchor_idx, 4]\n",
    "                    class_scores = output[row, col, anchor_idx, 5:]\n",
    "                    \n",
    "                    # Only process boxes with confidence above threshold\n",
    "                    if confidence < confidence_threshold:\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert to absolute coordinates\n",
    "                    anchor_idx_global = output_idx * 3 + anchor_idx\n",
    "                    anchor_w, anchor_h = self.anchors[anchor_idx_global]\n",
    "                    \n",
    "                    x = (col + tx) / grid_size\n",
    "                    y = (row + ty) / grid_size\n",
    "                    w = np.exp(tw) * anchor_w / self.img_size[0]\n",
    "                    h = np.exp(th) * anchor_h / self.img_size[1]\n",
    "                    \n",
    "                    # Convert to corners format\n",
    "                    x1 = max(0, (x - w/2) * orig_width)\n",
    "                    y1 = max(0, (y - h/2) * orig_height)\n",
    "                    x2 = min(orig_width, (x + w/2) * orig_width)\n",
    "                    y2 = min(orig_height, (y + h/2) * orig_height)\n",
    "                    \n",
    "                    # Get class with highest score\n",
    "                    class_id = np.argmax(class_scores)\n",
    "                    class_score = class_scores[class_id]\n",
    "                    \n",
    "                    # Store detection\n",
    "                    detections.append({\n",
    "                        'class_id': class_id,\n",
    "                        'class_name': self.classes[class_id],\n",
    "                        'confidence': float(confidence * class_score),\n",
    "                        'box': [int(x1), int(y1), int(x2), int(y2)]\n",
    "                    })\n",
    "    \n",
    "    # Apply non-maximum suppression\n",
    "    return self._non_max_suppression(detections, 0.45)\n",
    "\n",
    "def _non_max_suppression(self, detections, iou_threshold=0.45):\n",
    "    \"\"\"\n",
    "    Apply non-maximum suppression to remove overlapping detections\n",
    "    \n",
    "    Parameters:\n",
    "    detections (list): List of detections\n",
    "    iou_threshold (float): IoU threshold for considering boxes as overlapping\n",
    "    \n",
    "    Returns:\n",
    "    list: Filtered list of detections\n",
    "    \"\"\"\n",
    "    if not detections:\n",
    "        return []\n",
    "    \n",
    "    # Sort by confidence\n",
    "    detections.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # Apply NMS\n",
    "    filtered_detections = []\n",
    "    while detections:\n",
    "        current = detections.pop(0)\n",
    "        filtered_detections.append(current)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(detections):\n",
    "            if current['class_id'] == detections[i]['class_id'] and \\\n",
    "               self._calculate_iou(current['box'], detections[i]['box']) > iou_threshold:\n",
    "                detections.pop(i)\n",
    "            else:\n",
    "                i += 1\n",
    "    \n",
    "    return filtered_detections\n",
    "\n",
    "def _calculate_iou(self, box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU (Intersection over Union) between two boxes\n",
    "    \"\"\"\n",
    "    # Box format: [x1, y1, x2, y2]\n",
    "    x1_min, y1_min, x1_max, y1_max = box1\n",
    "    x2_min, y2_min, x2_max, y2_max = box2\n",
    "    \n",
    "    # Calculate intersection area\n",
    "    xi_min = max(x1_min, x2_min)\n",
    "    yi_min = max(y1_min, y2_min)\n",
    "    xi_max = min(x1_max, x2_max)\n",
    "    yi_max = min(y1_max, y2_max)\n",
    "    \n",
    "    if xi_max <= xi_min or yi_max <= yi_min:\n",
    "        return 0.0  # No intersection\n",
    "    \n",
    "    intersection_area = (xi_max - xi_min) * (yi_max - yi_min)\n",
    "    \n",
    "    # Calculate union area\n",
    "    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "    \n",
    "    return intersection_area / union_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualization Methods\n",
    "def visualize_detections(self, img_path, detections):\n",
    "    \"\"\"\n",
    "    Visualize detections on an image\n",
    "    \n",
    "    Parameters:\n",
    "    img_path (str): Path to the image file\n",
    "    detections (list): List of detections\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = plt.gca()\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    colors = {\n",
    "        'Hotspots': 'red',\n",
    "        'Cracks': 'green',\n",
    "        'Shadings': 'blue'\n",
    "    }\n",
    "    \n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = det['box']\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        \n",
    "        # Draw bounding box\n",
    "        rect = Rectangle((x1, y1), w, h, linewidth=2, \n",
    "                       edgecolor=colors[det['class_name']], \n",
    "                       facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label\n",
    "        plt.text(x1, y1 - 5, \n",
    "                f\"{det['class_name']} {det['confidence']:.2f}\", \n",
    "                color='white', fontsize=12, \n",
    "                bbox={'facecolor': colors[det['class_name']], 'alpha': 0.7, 'pad': 2})\n",
    "    \n",
    "    plt.title(f\"Solar Panel Fault Detection: {len(detections)} faults detected\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Thermal Image Processing Methods\n",
    "def process_thermal_image(self, img_path):\n",
    "    \"\"\"\n",
    "    Process a thermal image to enhance fault visibility\n",
    "    \n",
    "    Parameters:\n",
    "    img_path (str): Path to the thermal image\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Processed image\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "    \n",
    "    # Convert to grayscale if not already\n",
    "    if len(img.shape) > 2:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img.copy()\n",
    "    \n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced = clahe.apply(gray)\n",
    "    \n",
    "    # Apply false-color mapping for better visualization\n",
    "    colored = cv2.applyColorMap(enhanced, cv2.COLORMAP_JET)\n",
    "    \n",
    "    return colored\n",
    "\n",
    "def detect_and_analyze(self, img_path):\n",
    "    \"\"\"\n",
    "    Detect faults and analyze them in a thermal image\n",
    "    \n",
    "    Parameters:\n",
    "    img_path (str): Path to the image file\n",
    "    \"\"\"\n",
    "    # Process thermal image\n",
    "    processed_img = self.process_thermal_image(img_path)\n",
    "    \n",
    "    # Detect faults\n",
    "    detections = self.detect(img_path)\n",
    "    \n",
    "    # Count detections by type\n",
    "    counts = {'Hotspots': 0, 'Cracks': 0, 'Shadings': 0}\n",
    "    for det in detections:\n",
    "        counts[det['class_name']] += 1\n",
    "    \n",
    "    # Calculate severity scores\n",
    "    hotspot_severity = self._calculate_hotspot_severity(processed_img, detections)\n",
    "    crack_severity = self._calculate_crack_severity(processed_img, detections)\n",
    "    shading_severity = self._calculate_shading_severity(processed_img, detections)\n",
    "    \n",
    "    # Visualize\n",
    "    self.visualize_detections(img_path, detections)\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"\\nFault Analysis:\")\n",
    "    print(f\"Hotspots: {counts['Hotspots']} detected (Severity: {hotspot_severity:.2f}%)\")\n",
    "    print(f\"Cracks: {counts['Cracks']} detected (Severity: {crack_severity:.2f}%)\")\n",
    "    print(f\"Shadings: {counts['Shadings']} detected (Severity: {shading_severity:.2f}%)\")\n",
    "    \n",
    "    # Overall severity\n",
    "    total_severity = (hotspot_severity + crack_severity + shading_severity) / 3\n",
    "    print(f\"\\nOverall Panel Health: {100 - total_severity:.2f}%\")\n",
    "    \n",
    "    if total_severity > 50:\n",
    "        print(\"Recommendation: Immediate maintenance required\")\n",
    "    elif total_severity > 25:\n",
    "        print(\"Recommendation: Schedule maintenance soon\")\n",
    "    else:\n",
    "        print(\"Recommendation: Normal operation, no immediate action required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Severity Calculation Methods\n",
    "def _calculate_hotspot_severity(self, img, detections):\n",
    "    \"\"\"Calculate severity score for hotspots\"\"\"\n",
    "    hotspots = [d for d in detections if d['class_name'] == 'Hotspots']\n",
    "    if not hotspots:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate based on area and intensity\n",
    "    severity = 0\n",
    "    img_area = img.shape[0] * img.shape[1]\n",
    "    \n",
    "    for det in hotspots:\n",
    "        x1, y1, x2, y2 = det['box']\n",
    "        hotspot_area = (x2 - x1) * (y2 - y1)\n",
    "        area_ratio = hotspot_area / img_area\n",
    "        \n",
    "        # Extract the hotspot region and calculate intensity\n",
    "        roi = img[y1:y2, x1:x2]\n",
    "        if roi.size > 0:\n",
    "            # For thermal images, higher values (brighter) indicate hotter areas\n",
    "            intensity = np.mean(roi) / 255.0\n",
    "            severity += area_ratio * intensity * 100 * 2  # Weight factor\n",
    "    \n",
    "    return min(severity, 100)  # Cap at 100%\n",
    "\n",
    "def _calculate_crack_severity(self, img, detections):\n",
    "    \"\"\"Calculate severity score for cracks\"\"\"\n",
    "    cracks = [d for d in detections if d['class_name'] == 'Cracks']\n",
    "    if not cracks:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate based on length and location\n",
    "    severity = 0\n",
    "    img_diagonal = np.sqrt(img.shape[0]**2 + img.shape[1]**2)\n",
    "    \n",
    "    for det in cracks:\n",
    "        x1, y1, x2, y2 = det['box']\n",
    "        # Estimate crack length as box diagonal\n",
    "        crack_length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "        length_ratio = crack_length / img_diagonal\n",
    "        \n",
    "        # Location factor - cracks in center are more severe\n",
    "        center_x = (x1 + x2) / 2\n",
    "        center_y = (y1 + y2) / 2\n",
    "        distance_to_center = np.sqrt(\n",
    "            (center_x - img.shape[1]/2)**2 + \n",
    "            (center_y - img.shape[0]/2)**2\n",
    "        )\n",
    "        distance_ratio = 1 - (distance_to_center / (img_diagonal/2))\n",
    "        \n",
    "        severity += length_ratio * distance_ratio * 100 * 1.5  # Weight factor\n",
    "    \n",
    "    return min(severity, 100)  # Cap at 100%\n",
    "\n",
    "def _calculate_shading_severity(self, img, detections):\n",
    "    \"\"\"Calculate severity score for shadings\"\"\"\n",
    "    shadings = [d for d in detections if d['class_name'] == 'Shadings']\n",
    "    if not shadings:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate based on area coverage\n",
    "    severity = 0\n",
    "    img_area = img.shape[0] * img.shape[1]\n",
    "    \n",
    "    for det in shadings:\n",
    "        x1, y1, x2, y2 = det['box']\n",
    "        shading_area = (x2 - x1) * (y2 - y1)\n",
    "        area_ratio = shading_area / img_area\n",
    "        \n",
    "        # Extract region to analyze temperature difference\n",
    "        roi = img[y1:y2, x1:x2]\n",
    "        if roi.size > 0:\n",
    "            # For shading, calculate temperature difference (lower values in thermal images)\n",
    "            avg_temp = np.mean(roi)\n",
    "            avg_img_temp = np.mean(img)\n",
    "            temp_diff_ratio = abs(avg_temp - avg_img_temp) / 255.0\n",
    "            \n",
    "            severity += area_ratio * temp_diff_ratio * 100\n",
    "    \n",
    "    return min(severity, 100)  # Cap at 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Now let's demonstrate how to use our Solar Panel YOLO Detector with some example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define the path to your dataset\n",
    "data_dir = \"./solar_panel_dataset\"  # Update this to your dataset path\n",
    "\n",
    "# Initialize the detector\n",
    "detector = SolarPanelYOLODetector(data_dir)\n",
    "\n",
    "# Load the dataset\n",
    "train_images, train_annotations, test_images, test_annotations = detector.load_yolo_dataset(split_ratio=0.8)\n",
    "\n",
    "# Build the model\n",
    "model = detector.build_yolo_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train the model (commented out by default as it can take a long time)\n",
    "# Uncomment to train\n",
    "\n",
    "# history = detector.train_model(\n",
    "#     train_images, \n",
    "#     train_annotations, \n",
    "#     batch_size=8, \n",
    "#     epochs=50, \n",
    "#     validation_split=0.1\n",
    "# )\n",
    "\n",
    "# # Plot training history\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Model Loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Sample Images\n",
    "\n",
    "To use this on your own images, you'll need to have a trained model. For demonstration purposes, we'll assume you have a pre-trained model ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to test on sample images\n",
    "def test_on_sample(detector, image_path):\n",
    "    print(f\"Testing on image: {image_path}\")\n",
    "    \n",
    "    # 1. Process the thermal image\n",
    "    processed = detector.process_thermal_image(image_path)\n",
    "    \n",
    "    # Display the processed image\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(cv2.cvtColor(processed, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Processed Thermal Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Detect and analyze faults\n",
    "    detector.detect_and_analyze(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test on a sample image (update the path to your test image)\n",
    "# test_on_sample(detector, \"./solar_panel_dataset/images/sample_hotspot_01.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has implemented a YOLO-based solar panel fault detection system that can:\n",
    "\n",
    "1. Process thermal images of solar panels\n",
    "2. Detect three types of faults: hotspots, cracks, and shadings\n",
    "3. Analyze the severity of detected faults\n",
    "4. Provide maintenance recommendations based on fault severity\n",
    "\n",
    "The model architecture used is a simplified version of YOLO that can be trained on a custom dataset of solar panel images. The system can be deployed as part of a maintenance workflow for solar installations to detect issues early and prevent power loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}